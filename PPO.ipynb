{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44033e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "import numpy as np\n",
    "import random\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from TestEnv import HydroElectric_Test\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from DQQN import DQNWrapper\n",
    "from reward_shaping import reward_shaping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e299fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOWrapper(gym.Env):\n",
    "    def __init__(self, env, reward_shape = True):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.max_episode_length = len(self.env.price_values.flatten())\n",
    "        # self.actions = np.linspace(-1.0, 1.0, num_actions)\n",
    "        self.action_space = Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "        self.reward_shape = reward_shape\n",
    "\n",
    "        # 5 discrete features\n",
    "        self.observation_space = Box(low=0.0, high=1.0, shape=(5,), dtype=np.float32)\n",
    "        self.action_history = []\n",
    "\n",
    "    def normalize(self, state): \n",
    "        state = state.astype(np.float32) \n",
    "        state[0] /= self.env.max_volume \n",
    "        state[1] /= np.max(self.env.price_values) \n",
    "        state[2] = (state[2]-1) /23.0 \n",
    "        state[3] /= 6.0 \n",
    "        state[5] = (state[5]-1)/ 12.0 \n",
    "\n",
    "        return np.array([state[0], state[1], state[2], state[3], state[5]],dtype=np.float32)\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, **kwargs):\n",
    "        self.env.counter = 0\n",
    "        self.env.hour = 1\n",
    "        self.env.day = 1\n",
    "        self.env.volume = self.env.max_volume / 2\n",
    "        self.action_history = []\n",
    "        # self.env.reset()\n",
    "        obs = np.array(self.env.observation(), dtype=np.float32)\n",
    "        return self.normalize(obs), {}\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        real_action = float(action[0])\n",
    "        self.action_history.append(real_action)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = self.env.step(real_action)\n",
    "        shaped_reward = reward\n",
    "\n",
    "        if self.reward_shape: \n",
    "            shaped_reward = reward_shaping(self.env, reward, self.action_history)\n",
    "\n",
    "        next_obs = self.normalize(np.array(next_obs, dtype=np.float32))\n",
    "\n",
    "        return next_obs, shaped_reward, terminated, truncated, info\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.env, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae4ed4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.63e+04     |\n",
      "|    ep_rew_mean          | -1.07e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 335          |\n",
      "|    iterations           | 100          |\n",
      "|    time_elapsed         | 610          |\n",
      "|    total_timesteps      | 204800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005406586 |\n",
      "|    clip_fraction        | 0.00645      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | -5.25e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.463        |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | 0.000904     |\n",
      "|    std                  | 0.897        |\n",
      "|    value_loss           | 0.477        |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "seed = 5\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "env = HydroElectric_Test(\"train.xlsx\")\n",
    "env = PPOWrapper(env, reward_shape=True)\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=300_000,log_interval=100)\n",
    "model.save(\"ppo_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8c9710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 47.07\n",
      "Mean reward: 0.003\n"
     ]
    }
   ],
   "source": [
    "env_test = HydroElectric_Test(\"validate.xlsx\")\n",
    "env_test = PPOWrapper(env_test, reward_shape=False)\n",
    "\n",
    "model = PPO.load(\"ppo_model\")\n",
    "obs, _ = env_test.reset()\n",
    "\n",
    "done = False\n",
    "water_levels = []\n",
    "rewards = []\n",
    "actions = []\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    \n",
    "    obs, reward, terminated, truncated, _ = env_test.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    water_levels.append(env.env.volume) \n",
    "    rewards.append(reward)\n",
    "    actions.append(action[0])\n",
    "\n",
    "print(\"Total reward:\", round(sum(rewards),2))\n",
    "print(\"Mean reward:\", round(np.mean(rewards),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f5195e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(set(water_levels)))\n",
    "# print(set(actions))\n",
    "print(len(set(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d915f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.float64(0.0),\n",
       " np.float64(12.1855012483932),\n",
       " np.float64(17.401972921567204),\n",
       " np.float64(17.481433985136)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248326ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
